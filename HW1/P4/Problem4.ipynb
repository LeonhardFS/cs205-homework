{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import matplotlib.ticker as ticker   \n",
    "\n",
    "\n",
    "# setup spark\n",
    "conf = SparkConf().setAppName('Graph Processing')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.DataFrameGroupBy object at 0x10d8b47d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comicdf = pd.read_csv('source.csv', names=['Character', 'ComicIssue'])\n",
    "comicdf.groupby('ComicIssue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keys = comicdf['Character'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = range(1, len(keys)+1) # start indices with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "characterDict = dict(zip(keys, values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characterDict['FROST, CARMILLA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create graph by adjacency list (there is a connection between\n",
    "# two characters if they appear in the same comic issue)\n",
    "\n",
    "# therefore join the comicdf on comic issue!\n",
    "mergeddf = pd.merge(comicdf, comicdf, how='inner', on='ComicIssue')\n",
    "\n",
    "# now remove ComicIssue & all rows with Character_x == Character_y\n",
    "filtereddf = mergeddf.drop('ComicIssue', 1)\n",
    "filtereddf = filtereddf[filtereddf['Character_x'] != filtereddf['Character_y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# transform string to integers for performance reasons\n",
    "edgedf = filtereddf.applymap(lambda x: characterDict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# save the edge list and the dictionary as two separate csv's\n",
    "edgedf.to_csv('edge_list.csv', header=False, index=False)\n",
    "\n",
    "writer = csv.writer(open('characters.csv', 'wb'))\n",
    "entries = sorted(characterDict.items(), key=lambda x: x[1]);\n",
    "for key, value in entries:\n",
    "    writer.writerow([value, key]) # flip it (so the vertex index is now the key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load vertex dictionary (is basically the character dictionary inverted)\n",
    "reader = csv.reader(open('characters.csv', 'rb'))\n",
    "vertexDict = dict(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementation after \n",
    "# http://www.johnandcailin.com/blog/cailin/breadth-first-graph-search-using-iterative-map-reduce-algorithm\n",
    "# now load data into spark!\n",
    "rdd = sc.textFile('edge_list_simple.csv')\n",
    "\n",
    "# to avoid overhead by checking everytime for gray nodes via collect()\n",
    "# we use an accumulator!\n",
    "num_gray_nodes = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# map string to tuples\n",
    "rdd = rdd.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda x: (int(x[0]), int(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 5), (2, 1), (2, 5), (2, 3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now group s.t. we have for each vertex an adjacency list of nodes\n",
    "rdd = rdd.groupByKey().map(lambda x: (x[0], list(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, [1, 5, 3, 4]), (4, [2, 5, 3]), (6, [7]), (1, [2, 5]), (3, [2, 4])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import functools\n",
    "# init bfs\n",
    "v0 = 1 # start vertex\n",
    "imaxvalue = sys.maxint\n",
    "\n",
    "# currently our rdd looks like (v, [v1, v2, ...]). now we map it to a tuple with\n",
    "# (v, <adj. list.>, disttov0, color) where disttov0 is 0 for v = v0 and imaxvalue else, color = GRAY for v0 and BLACK else\n",
    "# WHITE means vertex not visited yet\n",
    "# GRAY means vertex is visited in the next hop\n",
    "# BLACK mean vertex already visited\n",
    "\n",
    "# to speedup we use\n",
    "# WHITE = 2\n",
    "# GRAY = 1\n",
    "# BLACK = 0\n",
    "rdd = rdd.map(lambda x: (x[0], x[1], 0 if x[0] == v0 else imaxvalue, 1 if x[0] == v0 else 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, [1, 5, 3, 4], 9223372036854775807, 2),\n",
       " (4, [2, 5, 3], 9223372036854775807, 2),\n",
       " (6, [7], 9223372036854775807, 2),\n",
       " (1, [2, 5], 0, 1),\n",
       " (3, [2, 4], 9223372036854775807, 2)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now we perform one hop, i.e. we expand all gray notes\n",
    "# say we have given (v, [v_1, ..., v_d], d, 'GRAY')\n",
    "# this will be expanded to \n",
    "# (v_1, NULL, d+1, 'GRAY')\n",
    "# ...\n",
    "# (v_d, NULL, d+1, 'GRAY')\n",
    "# (v, [v_1, ..., v_d], d, 'BLACK')\n",
    "# in the next step we can then call a reducebykey to update distances/adjacency lists\n",
    "def expandNode(x):\n",
    "    if x[3] == 1: # 'GRAY'\n",
    "    # set current node to visited\n",
    "        res = []\n",
    "        res.append( (x[0], x[1], x[2], 0) ) # 'BLACK'\n",
    "\n",
    "        # spawn new GRAY nodes\n",
    "        for i in range(0, len(x[1])):\n",
    "            res.append( (x[1][i], [], x[2] + 1, 1) ) # 'GRAY'\n",
    "\n",
    "        return tuple(res)\n",
    "    else: \n",
    "        return [x]\n",
    "    \n",
    "rdd = rdd.flatMap(expandNode);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, [1, 5, 3, 4], 9223372036854775807, 2),\n",
       " (4, [2, 5, 3], 9223372036854775807, 2),\n",
       " (6, [7], 9223372036854775807, 2),\n",
       " (1, [2, 5], 0, 0),\n",
       " (2, [], 1, 1),\n",
       " (5, [], 1, 1),\n",
       " (3, [2, 4], 9223372036854775807, 2),\n",
       " (5, [4, 1, 2], 9223372036854775807, 2),\n",
       " (7, [6], 9223372036854775807, 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# in the next step we combine all tuples for the same key returning the minimum distance, longest adjacency list and darkest color\n",
    "# the algorithm will determine if there is no gray node left\n",
    "def reduceNodes(a, b):\n",
    "    res = (a[0] if len(a[0]) > len(b[0]) else b[0], \\\n",
    "            min(a[1], b[1]), min(a[2], b[2]))\n",
    "    \n",
    "    if res[2] == 1:\n",
    "        num_gray_nodes.add(1) # inc count of remaining gray nodes by 1!\n",
    "    \n",
    "    # return a tuple of 3 entries\n",
    "    return res\n",
    "    \n",
    "    \n",
    "# map first to a (key, value) pair\n",
    "rdd = rdd.map(lambda x: (x[0], (x[1], x[2], x[3])))\n",
    "\n",
    "# then reduce by key\n",
    "rdd = rdd.reduceByKey(reduceNodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, ([1, 5, 3, 4], 1, 1)),\n",
       " (4, ([2, 5, 3], 9223372036854775807, 2)),\n",
       " (6, ([7], 9223372036854775807, 2)),\n",
       " (1, ([2, 5], 0, 0)),\n",
       " (3, ([2, 4], 9223372036854775807, 2))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_gray_nodes.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# put the whole process into a function, given an rdd which holds as elements\n",
    "# (v, [v_1, ..., v_d]),  a start node v0 and the spark context corresponding to the rdd\n",
    "def sparkBFS(context, rdd, v0):\n",
    "    \n",
    "    # prepare data structure for single source shortest paths\n",
    "    imaxvalue = sys.maxint\n",
    "\n",
    "    # currently our rdd looks like (v, [v1, v2, ...]). now we map it to a tuple with\n",
    "    # (v, <adj. list.>, disttov0, color) where disttov0 is 0 for v = v0 and imaxvalue else,\n",
    "    # color = GRAY for v0 and BLACK else\n",
    "    # WHITE means vertex not visited yet\n",
    "    # GRAY means vertex is visited in the next hop\n",
    "    # BLACK mean vertex already visited\n",
    "\n",
    "    # to speedup we use\n",
    "    # WHITE = 2\n",
    "    # GRAY = 1\n",
    "    # BLACK = 0\n",
    "    rdd = rdd.map(lambda x: (x[0], \\\n",
    "                             x[1], \\\n",
    "                             0 if x[0] == v0 else imaxvalue, \\\n",
    "                             1 if x[0] == v0 else 2))\n",
    "    \n",
    "    # map to (K, V) form\n",
    "    rdd = rdd.map(lambda x: (x[0], (x[1], x[2], x[3])))\n",
    "    \n",
    "    # helper functions for one hop START\n",
    "    \n",
    "    # say we have given (v, [v_1, ..., v_d], d, 'GRAY')\n",
    "    # this will be expanded to \n",
    "    # (v_1, NULL, d+1, 'GRAY')\n",
    "    # ...\n",
    "    # (v_d, NULL, d+1, 'GRAY')\n",
    "    # (v, [v_1, ..., v_d], d, 'BLACK')\n",
    "    # in the next step we can then call a reducebykey to update distances/adjacency lists\n",
    "    def expandNode(x):\n",
    "        if x[1][2] == 1: # 'GRAY'\n",
    "        # set current node to visited\n",
    "            res = []\n",
    "            res.append( (x[0], (x[1][0], x[1][1], 0)) ) # 'BLACK'\n",
    "\n",
    "            # spawn new GRAY nodes\n",
    "            for i in range(0, len(x[1][0])):\n",
    "                res.append( (x[1][0][i], ([], x[1][1] + 1, 1)) ) # 'GRAY'\n",
    "\n",
    "            return tuple(res)\n",
    "        else: \n",
    "            return [x]\n",
    "        \n",
    "    # in the next step we combine all tuples for the same key returning\n",
    "    # the minimum distance, longest adjacency list and darkest color\n",
    "    # the algorithm will determine if there is no gray node left\n",
    "    def reduceNodes(a, b, gray_accum):\n",
    "        res = (a[0] if len(a[0]) > len(b[0]) else b[0], \\\n",
    "                min(a[1], b[1]), min(a[2], b[2]))\n",
    "\n",
    "        if res[2] == 1:\n",
    "            gray_accum.add(1) # inc count of remaining gray nodes by 1!\n",
    "\n",
    "        # return a tuple of 3 entries\n",
    "        return res\n",
    "    \n",
    "    # helper functions END\n",
    "    \n",
    "    # set num_gray_nodes to 1 to start loop (finished when all nodes are visited)\n",
    "    num_remaining_gray_nodes = 1;\n",
    "    num_visited_nodes = 0;\n",
    "    \n",
    "    rdd.take(5)\n",
    "    \n",
    "    counter = 0\n",
    "    gray_accum = 0\n",
    "    while num_remaining_gray_nodes > 0:# and counter < 2:\n",
    "    \n",
    "        # (1) set accumulator for gray nodes to zero\n",
    "        gray_accum = sc.accumulator(0)\n",
    "        \n",
    "        # (2) start map process\n",
    "        rdd = rdd.flatMap(expandNode);\n",
    "\n",
    "        # (3) then reduce by key\n",
    "        rdd = rdd.reduceByKey(functools.partial(reduceNodes, gray_accum=gray_accum))\n",
    "\n",
    "        # call count to evaluate accumulator correctly\n",
    "        rdd.count()\n",
    "        \n",
    "        # save value of gray node accumulator\n",
    "        num_remaining_gray_nodes = gray_accum.value\n",
    "        num_visited_nodes += num_remaining_gray_nodes\n",
    "            \n",
    "        counter += 1\n",
    "        \n",
    "    \n",
    "    # return number of visited nodes and the rdd\n",
    "    return num_visited_nodes, rdd\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIER NOCH AUF (KEY, VALUE) UMSTELLEN UM SINNLOSES MAPPING ZU SPAREN!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this function prepares the rdd\n",
    "def prepare_rdd(filename):\n",
    "    rdd = sc.textFile(filename)\n",
    "\n",
    "    # map string to tuples\n",
    "    rdd = rdd.map(lambda x: x.split(','))\n",
    "    rdd = rdd.map(lambda x: (int(x[0]), int(x[1])))\n",
    "    \n",
    "    # now group s.t. we have for each vertex an adjacency list of nodes\n",
    "    rdd = rdd.groupByKey().map(lambda x: (x[0], list(x[1])))\n",
    "    \n",
    "    return rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test code\n",
    "filename = 'edge_list_simple.csv' # 'edge_list.csv'\n",
    "\n",
    "rdd = prepare_rdd(filename)\n",
    "rdd.take(5)\n",
    "\n",
    "num_visited_nodes, rdd = sparkBFS(sc, rdd, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, ([1, 5, 3, 4], 1, 0)),\n",
       " (4, ([2, 5, 3], 2, 0)),\n",
       " (6, ([7], 9223372036854775807, 2)),\n",
       " (1, ([2, 5], 0, 0)),\n",
       " (3, ([2, 4], 2, 0)),\n",
       " (5, ([4, 1, 2], 1, 0)),\n",
       " (7, ([6], 9223372036854775807, 2))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_visited_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
